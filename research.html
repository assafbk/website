<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <title>Assaf Ben-Kish</title>
  <meta content="" name="descriptison">
  <meta content="" name="keywords">

  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.4/css/all.css" integrity="sha384-DyZ88mC6Up2uqS4h/KRgHuoeGwBcD4Ng9SiP4dIRy0EXTlnuz47vAwmeGwVChigm" crossorigin="anonymous"/> 
  <!-- Custom fonts for this theme -->
  <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Dosis:400,700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">

  <!-- Favicons -->
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
  <link rel="icon" href="favicon.ico" type="image/x-icon">

  <!-- Theme CSS -->
  <link href="css/freelancer.css" rel="stylesheet">

</head>

<body id="page-top">

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg bg-secondary fixed-top" id="mainNav">
    <div class="container">
      <a class="navbar-brand js-scroll-trigger" href="index.html">Assaf Ben-Kish</a>
      <button class="navbar-toggler navbar-toggler-right text-uppercase bg-secondary text-white rounded" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        Menu
        <i class="fas fa-bars"></i>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item mx-0 mx-lg-1">
            <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger large" href="about.html">About</a>
          </li>
          <!-- <li class="nav-item mx-0 mx-lg-1">
            <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger large" href="art.html">Art</a>
          </li> -->
          <li class="nav-item mx-0 mx-lg-1">
            <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger large" href="research.html">Research</a>
          </li>
           <!-- <li class="nav-item mx-0 mx-lg-1">
            <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger large" href="talks.html">Talks</a>
          </li> -->
          <li class="nav-item mx-0 mx-lg-1">
            <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger large" href="CV_Assaf_BenKish.pdf">CV</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>


  <!-- Portfolio Section -->
  <section class="page-section portfolio" id="portfolio">
    <div class="container">

      <!-- Portfolio Section Heading -->
      <h1 class="page-section-heading text-center text-uppercase mb-6">Research</h1>

      
      <div class="row mb-6">

        <div class="col-md-10 col-lg-10">
          <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#InspirationTree">
            <img class="img-fluid" src="img/figure/mocha_method_scheme.png" alt="">
          </div>
          <div class="portfolio-label">MOCHa: Multi-Objective Reinforcement Mitigating Caption Hallucinations</div>
          <div class="portfolio-authors"><b>Assaf Ben-Kish</b>, Moran Yanuka, Morris Alper, Raja Giryes, Hadar Elor</div>
          <div class="portfolio-subtitle">Arxiv</div>
          <div>
            <input type="button" value="Paper" style="display: block; margin: 0 auto;">
          </div>
        </div>
        <!-- <div class="container">
            <div class="center">
              
              <button>Website</button>
            </div>
          </div> -->

        <div class="col-md-10 col-lg-10">
          <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#wordasimage">
            <img class="img-fluid" src="img/figure/KP_CBW_BF_2_wide.png" alt="">
          </div>
          <div class="portfolio-label">Constant-Beamwidth Linearly Constrained Minimum Variance Beamformer</div>
          <div class="portfolio-authors">Ariel Frank, <b>Assaf Ben-Kish</b>, Israel Cohen</div>
          <div class="portfolio-subtitle">EUSIPCO 2022</div>
          <div>
            <input type="button" value="Paper" style="display: block; margin: 0 auto;" onclick="window.location.href='https://ieeexplore.ieee.org/document/9909899'">
          </div>
        </div>  

        <div class="col-md-10 col-lg-10">
          <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#wordasimage">
            <img class="img-fluid" src="img/figure/ddpg_mp_2_wide.png" alt="">
          </div>
          <div class="portfolio-label">DDPG-MP Extension: Improvements For Model-Based Actor Updates</div>
          <div class="portfolio-authors"><b>Assaf Ben-Kish</b>, Tom Jurgenson</div>
          <div class="portfolio-subtitle">Final Project B</div>
          <div>
            <input type="button" value="Project Poster" style="display: block; margin: 0 auto;">
          </div>
        </div>

        <div class="col-md-10 col-lg-10">
          <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#wordasimage">
            <img class="img-fluid" src="img/figure/ddpgmp_1_wide.png" alt="">
          </div>
          <div class="portfolio-label">DDPG-MP Extension: Implementation For Full Dynamic Simulation</div>
          <div class="portfolio-authors"><b>Assaf Ben-Kish</b>, Uri Gadot, Tom Jurgenson</div>
          <div class="portfolio-subtitle">Final Project A</div>
          <div>
            <input type="button" value="Project Poster" style="display: block; margin: 0 auto;">
          </div>
        </div>

    </div>
  </section>




<!-- <div class="portfolio-modal modal fade" id="InspirationTree" tabindex="-1" role="dialog" aria-labelledby="InspirationTree" aria-hidden="true">
    <div class="modal-dialog modal-xl" role="document">
      <div class="modal-content">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">
            <i class="fas fa-times fa-xs"></i>
          </span>
        </button>
        <div class="modal-body">
          <div class="container">
            <div class="row justify-content-center">
              <div class="col-lg-10">
                <h1 class="portfolio-modal-title mb-4 text-center">Concept Decomposition for Visual Exploration and Inspiration</h1>
                <p class="font-italic mb-3"><b>Yael Vinker</b>, Andrey Voynov, Daniel Cohen-Or, Ariel Shamir</p>
                
                <div class="video-container mb-4">
                  <video width="100%" height="auto" id="tree" controls>
                    <source src="img/portfolio/inspirationTree_gif.mp4"
                    type="video/mp4">
                  </video>
                </div>

                <h6 class="large font-weight-bold"><highlight>Abstract: </highlight></h6>
                <p class="mb-3">A creative idea is often born from transforming, combining, and modifying ideas from existing visual examples capturing various concepts. However, one cannot simply copy the concept as a whole, and inspiration is achieved by examining certain aspects of the concept. Hence, it is often necessary to separate a concept into different aspects to provide new perspectives. In this paper, we propose a method to decompose a visual concept, represented as a set of images, into different visual aspects encoded in a hierarchical tree structure. We utilize large vision-language models and their rich latent space for concept decomposition and generation. Each node in the tree represents a sub-concept using a learned vector embedding injected into the latent space of a pretrained text-to-image model. We use a set of regularizations to guide the optimization of the embedding vectors encoded in the nodes to follow the hierarchical structure of the tree. Our method allows to explore and discover new concepts derived from the original one. The tree provides the possibility of endless visual sampling at each node, allowing the user to explore the hidden sub-concepts of the object of interest. The learned aspects in each node can be combined within and across trees to create new visual ideas, and can be used in natural language sentences to apply such aspects to new designs.
                 </p>
                <div class="large mb-2 font-weight-bold">
                  <a href="https://inspirationtree.github.io/inspirationtree/" target="_blank">
                    > <highlight>Project website</highlight>
                  </a>
                </div>

                <div class="large mb-2 font-weight-bold">
                  <a href="https://arxiv.org/abs/2305.18203" target="_blank">
                    > <highlight>Paper</highlight>
                  </a>
                </div>
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div> -->

<!-- 
<div class="portfolio-modal modal fade" id="wordasimage" tabindex="-1" role="dialog" aria-labelledby="wordasimage" aria-hidden="true">
    <div class="modal-dialog modal-xl" role="document">
      <div class="modal-content">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">
            <i class="fas fa-times fa-xs"></i>
          </span>
        </button>
        <div class="modal-body">
          <div class="container">
            <div class="row justify-content-center">
              <div class="col-lg-10">
                <h1 class="portfolio-modal-title mb-4 text-center">Word-As-Image for Semantic Typography</h1>
                <p class="font-italic mb-3">Shir Iluz*, <b>Yael Vinker*</b>, Amir Hertz, Daniel Berio, Daniel Cohen-Or, Ariel Shamir</p>
                
                <div class="video-container mb-4">
                  <video width="100%" height="auto" id="tree" controls>
                    <source src="img/portfolio/word-as-image-video.mp4"
                    type="video/mp4">
                  </video>
                </div>

                <h6 class="large font-weight-bold"><highlight>Abstract: </highlight></h6>
                <p class="mb-3">A word-as-image is a semantic typography technique where a word illustration presents a visualization of the meaning of the word, while also preserving its readability. We present a method to create word-as-image illustrations automatically. This task is highly challenging as it requires semantic understanding of the word and a creative idea of where and how to depict these semantics in a visually pleasing and legible manner. We rely on the remarkable ability of recent large pretrained language-vision models to distill textual concepts visually. We target simple, concise, black-and-white designs that convey the semantics clearly. We deliberately do not change the color or texture of the letters and do not use embellishments. Our method optimizes the outline of each letter to convey the desired concept, guided by a pretrained Stable Diffusion model. We incorporate additional loss terms to ensure the legibility of the text and the preservation of the style of the font. We show high quality and engaging results on numerous examples and compare to alternative techniques.
                 </p>
                <div class="large mb-2 font-weight-bold">
                  <a href="https://wordasimage.github.io/Word-As-Image-Page/" target="_blank">
                    > <highlight>Project website</highlight>
                  </a>
                </div>

                <div class="large mb-2 font-weight-bold">
                  <a href="https://arxiv.org/abs/2303.01818" target="_blank">
                    > <highlight>Paper</highlight>
                  </a>
                </div>
              
                <div class="large mb-2 font-weight-bold">
                  <a href="https://github.com/Shiriluz/Word-As-Image" target="_blank">
                    > <highlight>Code</highlight>
                  </a>
                </div>

                <div class="large mb-2 font-weight-bold">
                  <a href="https://huggingface.co/spaces/SemanticTypography/Word-As-Image" target="_blank">
                    > <highlight>Demo</highlight>
                  </a>
                </div>
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>


    <div class="portfolio-modal modal fade" id="attend&excite" tabindex="-1" role="dialog" aria-labelledby="attend&excite" aria-hidden="true">
    <div class="modal-dialog modal-xl" role="document">
      <div class="modal-content">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">
            <i class="fas fa-times fa-xs"></i>
          </span>
        </button>
        <div class="modal-body">
          <div class="container">
            <div class="row justify-content-center">
              <div class="col-lg-10">
                <h1 class="portfolio-modal-title mb-4 text-center">Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models</h1>
                <p class="font-italic mb-3">Hila Chefer*, Yuval Alaluf*, <b>Yael Vinker</b>, Lior Wolf, Daniel Cohen-Or</p>

                <div class="video-container mb-4">
                  <video width="100%" height="auto" id="tree" controls>
                    <source src="img/portfolio/attend_and_excite_vid.mp4"
                    type="video/mp4">
                  </video>
                </div>
                <h6 class="large font-weight-bold"><highlight>Abstract: </highlight></h6>
                <p class="mb-3">Recent text-to-image generative models have demonstrated an unparalleled ability to generate diverse and creative imagery guided by a target text prompt. While revolutionary, current state-of-the-art diffusion models may still fail in generating images that fully convey the semantics in the given text prompt. We analyze the publicly available Stable Diffusion model and assess the existence of catastrophic neglect, where the model fails to generate one or more of the subjects from the input prompt. Moreover, we find that in some cases the model also fails to correctly bind attributes (e.g. colors) to their corresponding subjects. To help mitigate these failure cases, we introduce the concept of Generative Semantic Nursing (GSN), where we seek to intervene in the generative process on the fly during inference time to improve the faithfulness of the generated images. Using an attention- based formulation of GSN, dubbed Attend-and-Excite, we guide the model to refine the cross-attention units to attend to all subject tokens in the text prompt and strengthen — or excite — their activations, encouraging the model to generate all subjects described in the text prompt. We compare our approach to alternative approaches and demonstrate that it conveys the desired concepts more faithfully across a range of text prompts.
                 </p>
                <div class="large mb-2 font-weight-bold">
                  <a href="https://yuval-alaluf.github.io/Attend-and-Excite/" target="_blank">
                    > <highlight>Project website</highlight>
                  </a>
                </div>

                <div class="large mb-2 font-weight-bold">
                  <a href="https://arxiv.org/abs/2301.13826" target="_blank">
                    > <highlight>Paper</highlight>
                  </a>
                </div>
              
                <div class="large mb-2 font-weight-bold">
                  <a href="https://github.com/yuval-alaluf/Attend-and-Excite" target="_blank">
                    > <highlight>Code</highlight>
                  </a>
                </div>
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>

    <div class="portfolio-modal modal fade" id="clipascene" tabindex="-1" role="dialog" aria-labelledby="clipascene" aria-hidden="true">
    <div class="modal-dialog modal-xl" role="document">
      <div class="modal-content">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">
            <i class="fas fa-times fa-xs"></i>
          </span>
        </button>
        <div class="modal-body">
          <div class="container">
            <div class="row justify-content-center">
              <div class="col-lg-10">
                <h1 class="portfolio-modal-title mb-4 text-center">CLIPascene: Scene Sketching with Different Types and Levels of Abstraction</h1>
                <p class="font-italic mb-3"><b>Yael Vinker</b>, Yuval Alaluf, Daniel Cohen-Or, Ariel Shamir</p>
                
                <div class="video-container mb-4">
                  <video width="100%" height="auto" id="tree" controls>
                    <source src="img/portfolio/clipascene_video1.mp4"
                    type="video/mp4">
                  </video>
                </div>

                <h6 class="large font-weight-bold"><highlight>Abstract: </highlight></h6>
                <p class="mb-3">In this paper, we present a method for converting a given scene image into a sketch using different types and multiple levels of abstraction. We distinguish between two types of abstraction.
                The first considers the fidelity of the sketch, varying its representation from a more precise portrayal of the input to a looser depiction.
                The second is defined by the visual simplicity of the sketch, moving from a detailed depiction to a sparse sketch.
                Using an explicit disentanglement into two abstraction axes - and multiple levels for each one - provides users additional control over selecting the desired sketch based on their personal goals and preferences.
                To form a sketch at a given level of fidelity and simplification, we train two MLP networks. The first network learns the desired placement of strokes, while the second network learns to gradually remove strokes from the sketch without harming its recognizability and semantics.
                Our approach is able to generate sketches of complex scenes including those with complex backgrounds (e.g., natural and urban settings) and subjects (e.g., animals and people) while depicting gradual abstractions of the input scene in terms of fidelity and simplicity.
                 </p>
                <div class="large mb-2 font-weight-bold">
                  <a href="https://clipascene.github.io/CLIPascene/" target="_blank">
                    > <highlight>Project website</highlight>
                  </a>
                </div>

                <div class="large mb-2 font-weight-bold">
                  <a href="https://arxiv.org/abs/2211.17256" target="_blank">
                    > <highlight>Paper</highlight>
                  </a>
                </div>
              
                <div class="large mb-2 font-weight-bold">
                  <a href="https://github.com/yael-vinker/SceneSketch" target="_blank">
                    > <highlight>Code</highlight>
                  </a>
                </div>
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>

  <div class="portfolio-modal modal fade" id="clipasso" tabindex="-1" role="dialog" aria-labelledby="wheelchairLabel" aria-hidden="true">
    <div class="modal-dialog modal-xl" role="document">
      <div class="modal-content">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">
            <i class="fas fa-times fa-xs"></i>
          </span>
        </button>
        <div class="modal-body">
          <div class="container">
            <div class="row justify-content-center">
              <div class="col-lg-10">
                <h1 class="portfolio-modal-title mb-4 text-center">CLIPasso: Semantically-Aware Object Sketching</h1>
                <p class="font-italic mb-3"><b>Yael Vinker</b>, Ehsan Pajouheshgar, Jessica Y. Bo, Roman Bachmann, Amit Haim Bermano, Daniel Cohen-Or, Amir Zamir, Ariel Shamir</p>
                
                <div class="video-container mb-4">
                  <video width="100%" height="auto" id="tree" controls>
                    <source src="img/portfolio/clipasso_vid.mp4"
                    type="video/mp4">
                  </video>
                </div>

                <h6 class="large font-weight-bold"><highlight>Abstract: </highlight></h6>
                <p class="mb-3">Abstraction is at the heart of sketching due to the simple and minimal nature of line drawings. Abstraction entails identifying the essential visual properties of an object or scene, which requires semantic understanding and prior knowledge of high-level concepts. Abstract depictions are therefore challenging for artists, and even more so for machines. We present an object sketching method that can achieve different levels of abstraction, guided by geometric and semantic simplifications. While sketch generation methods often rely on explicit sketch datasets for training, we utilize the remarkable ability of CLIP (Contrastive-Language-Image-Pretraining) to distill semantic concepts from sketches and images alike. We define a sketch as a set of Bézier curves and use a differentiable rasterizer to optimize the parameters of the curves directly with respect to a CLIP-based perceptual loss. The abstraction degree is controlled by varying the number of strokes. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual components of the subject drawn. </p>
                <div class="large mb-2 font-weight-bold">
                  <a href="https://clipasso.github.io/clipasso/" target="_blank">
                    > <highlight>Project website</highlight>
                  </a>
                </div>

                <div class="large mb-2 font-weight-bold">
                  <a href="https://arxiv.org/abs/2202.05822" target="_blank">
                    > <highlight>Paper</highlight>
                  </a>
                </div>
              
                <div class="large mb-2 font-weight-bold">
                  <a href="https://github.com/yael-vinker/CLIPasso" target="_blank">
                    > <highlight>Code</highlight>
                  </a>
                </div>
                <div class="large mb-2 font-weight-bold">
                  <a href="https://replicate.com/yael-vinker/clipasso" target="_blank">
                    > <highlight>Demo</highlight>
                  </a>
                </div>
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>

  <div class="portfolio-modal modal fade" id="deepsim" tabindex="-1" role="dialog" aria-labelledby="visionLabel" aria-hidden="true">
    <div class="modal-dialog modal-xl" role="document">
      <div class="modal-content">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">
            <i class="fas fa-times fa-xs"></i>
          </span>
        </button>
        <div class="modal-body">
          <div class="container">
            <div class="row justify-content-center">
              <div class="col-lg-10">
                <h1 class="portfolio-modal-title mb-4 text-center">DeepSIM: Image Shape Manipulation from a Single Augmented Training Sample</h1>
                <p class="font-italic mb-3">Yael Vinker*, Eliahu Horwitz*, Nir Zabari , Yedid Hoshen</p>
                <img class="img-fluid rounded mb-3" src="img/portfolio/3dvision_img.png" alt="">
                
                
                <div class="video-container mb-4">
                  <iframe width="560" height="315" src="https://www.youtube.com/watch?v=RZwnnttQYzs&t=5s&ab_channel=EliahuHorwitz" frameborder="1" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div>

                <h6 class="large font-weight-bold"><highlight>Abstract: </highlight></h6>
                <p>We present DeepSIM, a generative model for conditional image manipulation based on a single image. We find that extensive augmentation is key for enabling single image training, and incorporate the use of thin-plate-spline (TPS) as an effective augmentation. Our network learns to map between a primitive representation of the image to the image itself. The choice of a primitive representation has an impact on the ease and expressiveness of the manipulations and can be automatic (e.g. edges), manual (e.g. segmentation) or hybrid such as edges on top of segmentations. At manipulation time, our generator allows for making complex image changes by modifying the primitive input representation and mapping it through the network. Our method is shown to achieve remarkable performance on image manipulation tasks.</p>

                <div class="large mb-2 font-weight-bold">
                  <a href="https://www.vision.huji.ac.il/deepsim/" target="_blank">
                    > <highlight>Project website</highlight>
                  </a>
                </div>

                <div class="large mb-2 font-weight-bold">
                  <a href="https://arxiv.org/pdf/2007.01289.pdf" target="_blank">
                    > <highlight>Paper</highlight>
                  </a>
                </div>
              
                <div class="large mb-2 font-weight-bold">
                  <a href="https://github.com/eliahuhorwitz/DeepSIM" target="_blank">
                    > <highlight>Code</highlight>
                  </a>
                </div>
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>


  <div class="portfolio-modal modal fade" id="hdr" tabindex="-1" role="dialog" aria-labelledby="visionLabel" aria-hidden="true">
    <div class="modal-dialog modal-xl" role="document">
      <div class="modal-content">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">
            <i class="fas fa-times fa-xs"></i>
          </span>
        </button>
        <div class="modal-body">
          <div class="container">
            <div class="row justify-content-center">
              <div class="col-lg-10">
                <h1 class="portfolio-modal-title mb-4 text-center">Unpaired Learning for High Dynamic Range Image Tone Mapping</h1>
                <p class="font-italic mb-3">Yael Vinker, Inbar Huberman-Spiegelglas, Raanan Fattal</p>
                <img class="img-fluid rounded mb-3" src="img/portfolio/3dvision_img.png" alt="">
                
                

                
                <div class="video-container mb-4">
                  <iframe width="560" height="315" src="https://youtu.be/v2r40TSRr3s" frameborder="1" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div>

                <h6 class="large font-weight-bold"><highlight>Abstract: </highlight></h6>
                <p>High dynamic range (HDR) photography is becoming increasingly popular and available by DSLR and mobile-phone cameras. While deep neural networks (DNN) have greatly impacted other domains of image manipulation, their use for HDR tone-mapping is limited due to the lack of a definite notion of ground-truth solution, which is needed for producing training data. In this paper we describe a new tone-mapping approach guided by the distinct goal of producing low dynamic range (LDR) renditions that best reproduce the visual characteristics of native LDR images. This goal enables the use of an unpaired adversarial training based on unrelated sets of HDR and LDR images, both of which are widely available and easy to acquire. In order to achieve an effective training under this minimal requirements, we introduce the following new steps and components: (i) a range-normalizing pre-process which estimates and applies a different level of curve-based compression, (ii) a loss that preserves the input content while allowing the network to achieve its goal, and (iii) the use of a more concise discriminator network, designed to promote the reproduction of low-level attributes native LDR possess. Evaluation of the resulting network demonstrates its ability to produce photo-realistic artifact-free tone-mapped images, and state-of-the-art performance on different image fidelity indices and visual distances.</p>

                <div class="large mb-2 font-weight-bold">
                  <a href="https://www.cs.huji.ac.il/w~raananf/projects/hdrgan/" target="_blank">
                    > <highlight>Project website</highlight>
                  </a>
                  <a href="https://arxiv.org/abs/2111.00219" target="_blank">
                    > <highlight>Paper</highlight>
                  </a>
                  <a href="https://github.com/yael-vinker/unpaired_hdr_tmo" target="_blank">
                    > <highlight>Code</highlight>
                  </a>
                </div>
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div> -->



<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>


</body>

</html>
